# Pattern | Claude都出4.5了，你还不懂200K到1M的上下文革命

你在Cursor里用Claude写代码，突然弹出"context window exceeded"？这不是你代码太多，而是你没理解200K上下文窗口的真实含义。200K tokens不等于200K字，一个汉字约等于2-3个tokens，一份10页的文档就可能用掉50K tokens。

本文将深入讲解Claude的上下文管理机制，回答这5个核心问题：

1.  200K上下文窗口到底能装多少内容？
2.  Prompt Caching怎么用？为什么有时候不生效？
3.  Context Editing会自动删掉我的重要内容吗？
4.  Claude 200K、GPT-4 128K和Gemini 1M，我到底该选谁？
5.  上下文窗口是不是越大越好？

---

## 一、上下文窗口的本质：不是字数限制，是工作记忆

### Token才是真正的计量单位

大部分人看到"200K上下文窗口"，第一反应是"能存20万个字"。这是最大的误解。

Token的真相是：英文1个token大约等于4个字符（比如`context`这个词就是1个token），中文1个token大约等于2-3个字符（`上下文`这三个字，可能就占了2-3个tokens），代码里你写的每一个符号、每一次缩进，都在消耗tokens。

所以200K tokens实际能装多少内容？大概是15万中文字，相当于75页Word文档。一份10页的PDF文档就要吃掉5万tokens，一个中等规模的项目代码库可能就占了10万tokens，再加上你和Claude的对话历史（每轮对话2-5万tokens），很快就接近极限了。

Claude Sonnet 4.5的200K窗口，实际可用空间只有约180K，因为要留出2万tokens给AI输出。在Cursor里写代码时，这个窗口装的是：你的项目代码 + 对话历史 + 工具调用记录。当这些加起来接近180K，"context window exceeded"就会弹出来打你脸。

### 工作记忆 vs 长期记忆

上下文窗口不是"记忆"，更准确的说法是"工作记忆"。

类比人脑就好理解了：工作记忆是你正在思考的信息，容量有限但处理速度快；长期记忆是你过去学过的知识，容量无限但提取需要时间。Claude的200K上下文窗口就像你的工作记忆，一旦对话结束或超出限制，内容就清空了。这就是为什么Anthropic推出了Memory Tool，让Claude拥有跨对话的"长期记忆"。

### 200K vs 1M的技术突破

Claude提供两种上下文窗口：200K是标准配置，适用于Sonnet 4.5、Opus 4这些主流模型，成本正常，性能最均衡。1M是Beta功能，只对Tier 4组织开放，容量是200K的5倍，但超过200K的部分要溢价2倍（Input $6/MTok，Output $22.5/MTok），还需要在API header里加上`context-1m-2025-08-07`。

1M上下文窗口是技术突破没错，但不是万能药。超长上下文会带来"Lost in the Middle"问题——研究表明上下文越长，模型对中间部分信息的注意力权重越低，容易漏掉关键内容。

---

## 二、省钱省力的两大核心技术

### Prompt Caching：省90%成本的杀手级功能

Prompt Caching的核心逻辑很简单：把不变的内容（工具定义、系统指令、代码库）标记为cache，首次计费正常价格，后续读取缓存只要0.1倍成本。

举个例子，你有个10万tokens的代码库，不用缓存的话，每次对话都要计算10万 × $3/MTok = $0.3。用了缓存后，首次$0.3，后续每次只要$0.03，直接省90%。

但很多人发现Prompt Caching经常不生效，原因无非就那几个。最常见的是**Token不足**，Opus/Sonnet要求至少1024 tokens，Haiku更是要2048 tokens，你的静态内容不够这个量，缓存根本不会触发。其次，缓存的检查范围也有限，只检查最近的**20个content blocks**，如果你缓存的内容被挤到这范围之外，也就检查不到了。缓存本身也有**5分钟的默认保质期**（TTL），超时不用就会过期，当然每次使用都会刷新这个时间。最后，一个常见的坑是**cache breakpoint设置错误**，它最多只支持4个，而且必须严格按照`tools → system → messages`的顺序设置，搞乱了顺序缓存就直接失效。

正确的策略是静态内容前置：把工具定义、系统指令、项目代码库这些不变的东西放前面标记cache，用户查询、对话历史这些频繁变化的放后面不缓存。Cursor就是这么干的：缓存整个项目代码库和.cursorrules配置，不缓存用户当前输入和最新对话历史。

### Context Editing：自动清理不是暴力删除

当你的对话接近200K限制时，Claude会自动触发Context Editing。很多人以为这是bug，其实是设计特性。

触发时机是上下文使用量超过180K tokens（留20K给输出），系统会自动判断哪些内容可以清理，但保留最近3次工具交互记录。这不是暴力删除，Anthropic设计了智能保留策略：最近3次工具调用记录肯定留着，System Prompt和最新对话轮次也会保留，清理的主要是早期对话历史、已经完成的任务记录、重复的上下文信息。

关键发现是Context Editing结合Memory Tool，性能能提升39%。这意味着Memory Tool会在清理前，自动把关键信息保存到长期记忆，清理后仍然可以recall回来。

如果你担心某些内容被清理怎么办？两个方法：一是让Memory Tool自动保存，清理后仍可recall；二是手动保存到项目文档，比如在Cursor里把关键代码或决策写到项目文件里，通过.cursorrules控制上下文范围，定期清理对话历史。

---

## 三、如何选择上下文方案

### 三大主流方案对比

| 维度 | Claude 200K | GPT-4 128K | Gemini 1M |
| :--- | :--- | :--- | :--- |
| 上下文容量 | 200K tokens | 128K tokens | 1M tokens |
| 性能均衡度 | ★★★★★ | ★★★★☆ | ★★★☆☆ |
| 成本 | 中等 | 中等 | 低 |
| 生态成熟度 | ★★★★☆ | ★★★★★ | ★★★☆☆ |
| Lost in the Middle | 较少 | 中等 | 严重 |

具体怎么选？日常开发用Claude 200K，Cursor、Claude Code这些开发工具配合中小型项目，成本和性能平衡最好。大型项目（10万行+代码库、长文档处理、多轮复杂对话）可以考虑Claude 1M Beta，前提是接受溢价成本。如果已有GPT应用迁移成本高，或者需要Plugin生态，就选GPT-4 128K。Gemini 1M谨慎选，适合免费额度试用，或者对中间内容不敏感的场景。

### 上下文不是越大越好

1M窗口比200K贵2倍（超过200K的部分），每次对话都计算全部tokens，没有Prompt Caching的话成本爆炸式增长。性能上也有代价："Lost in the Middle"让中间内容注意力权重下降，响应速度变慢（上下文越长推理越慢），质量也会下降（超长上下文容易产生幻觉）。工程复杂度更是大幅增加：上下文管理需要智能清理策略，cache breakpoint规划复杂，调试困难（问题定位难度增加）。

更好的替代方案是RAG + 中等窗口。RAG（检索增强生成）的优势是容量无限（外部知识库可以无限大），成本可控（只检索相关片段，不计算全部内容），质量更高（精准检索，避免"Lost in the Middle"）。

组合策略是这样的：长期知识用RAG检索，注入到200K上下文；实时对话用Context Editing，保持在200K内；关键记忆用Memory Tool，跨对话保留。Claude Code就是这么干的：用RAG索引整个代码库，200K上下文只装当前文件和最近对话，Memory Tool记住用户偏好和项目约定，Prompt Caching缓存项目配置。

---

## 快速解答

**Q1: 200K上下文窗口到底能装多少内容？**
A: 200K tokens约等于15万中文字或75页Word文档。英文1 token约4字符，中文1 token约2-3字符。实际可用约180K tokens，需预留20K给模型输出。

**Q2: Prompt Caching怎么用？为什么有时候不生效？**
A: 不生效的常见原因有Token量不足（Opus/Sonnet需1024+）、超出最近20个content blocks检查范围、缓存过期（5分钟TTL）或cache breakpoint顺序错误。最佳实践是缓存工具、代码库等静态内容。

**Q3: Context Editing会自动删掉我的重要内容吗？**
A: 不会暴力删除。系统会保留最近3次工具交互、System Prompt和最新对话。结合Memory Tool使用性能可提升39%，它会在清理前自动保存关键信息，不必担心丢失。

**Q4: Claude 200K、GPT-4 128K和Gemini 1M，我到底该选谁？**
A: 日常开发首选Claude 200K，性能最均衡；依赖插件生态选GPT-4 128K；大型项目且能接受溢价成本用Claude 1M Beta；Gemini 1M上下文最长但"Lost in the Middle"问题严重，需谨慎。

**Q5: 上下文窗口是不是越大越好？**
A: 不是。大窗口意味着成本翻倍、响应变慢、"Lost in the Middle"导致性能下降，工程复杂度也更高。目前最佳替代方案是 RAG + 200K中等窗口 + Memory Tool 的组合策略。

---

## 深入交流

想要深入交流AI实践经验？添加微信，一起探讨AI时代的无限可能！

![微信二维码](assets/二维码.jpg)

> ### 关于本人 ｜ 黄彦湘
> 深耕互联网行业9年，专注前端开发技术方向，现为广州执业律师，同时兼备专利代理师资质。基于丰富的技术背景和法律实践经验，现为深度实践（Deepractice）社区核心贡献者，致力于推动AI深度实践在法律、小说创作等多元领域的创新应用与探索。
> **全网同名**：明易AI实践

---
