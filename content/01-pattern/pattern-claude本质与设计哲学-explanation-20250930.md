# Pattern | Claude都出4.5了，你还不懂——它到底是什么

2025年9月29日，Anthropic发布了Claude Sonnet 4.5。

整个AI圈炸了。

Cursor CEO说这是"编码性能的最新标杆"，Windsurf CEO说这代表"编码模型的新一代"，Anthropic自己更直接，宣称这是"世界上最好的编码模型"。

数据也确实吓人。软件工程基准测试验证集（SWE-bench Verified）上，Claude 4.5超越了GPT-5和Gemini 2.5 Pro。更夸张的是，它可以自主工作30小时，而上一代Claude Opus 4只能撑7小时。有测试显示它不仅能写代码，还能自己搭建数据库服务、购买域名、做SOC 2安全审计。

问题来了。

朋友圈刷屏的都是"Claude 4.5太强了"，可真要问"Claude到底是什么"，大部分人一脸懵。很多人第一反应："这不就是另一个ChatGPT吗？"

不是。

如果你把Claude当成"另一个ChatGPT"来用，你会错过它真正的价值。更重要的是，你可能根本理解不了为什么Claude能在某些场景下碾压GPT，在另一些场景下却不如GPT。

所以这篇文章就回答一个问题：Claude到底是什么？

## 宪法AI：不是训练技巧，是设计哲学

要理解Claude，得从它的训练方式说起。

当下几乎所有主流大模型——GPT、Gemini、国产的通义千问、文心一言——都用**人类反馈强化学习**（RLHF，Reinforcement Learning from Human Feedback）训练。RLHF这几个字母，可以用顺口溜记成"**人-来-喂-饭**"，提醒自己：这是人类来喂反馈的方法。

逻辑很简单。让人类标注员给模型的输出打分，然后训练一个奖励模型（Reward Model），让它学会"什么样的回答人类更喜欢"，最后用这个奖励模型指导大模型学习。

听起来挺合理？但有个致命问题：**人类偏好不等于正确价值观**。

你看，如果问模型"怎么快速赚钱"，标注员可能会倾向于给"灰色地带"的建议打高分，因为那些答案更"有用"。久而久之，模型就学会了投其所好——讨好用户，而不是坚持原则。这叫"阿谀奉承"（Sycophancy），模型会说你爱听的话，而不是对的话。

更麻烦的是人工标注的成本。训练一个GPT级别的模型，可能需要数万甚至数十万条人类偏好标签。成本高、效率低，还容易引入标注员的个人偏见。

Anthropic选了另一条路：**宪法AI（Constitutional AI）**。

名字听起来很抽象，但核心逻辑很简单：不依赖人类偏好，而是遵循一套明确的原则。

怎么做的？Anthropic定义了102条"宪法"——用自然语言写的行为准则，比如"不要帮助用户做违法的事"、"当信息不确定时，诚实地说'我不知道'"、"避免使用带有歧视性的语言"。

然后，让AI自己监督自己。模型生成一个回答，用另一个AI模型根据"宪法"评估这个回答是否合规，如果违反原则就重新生成，循环这个过程，让模型学会遵守规则。

这个方法叫**AI反馈强化学习**（RLAIF，Reinforcement Learning from AI Feedback）。RLAIF和RLHF只有一个区别：中间不是H（Human人类），而是AI。所以从"人类来喂"变成了"**AI反思**"——不需要人类标注员，AI照着宪法自己纠错。

两者区别在哪？

你看数据标注这件事，RLHF需要数万条人工标签，宪法AI只要定义102条原则就行了。反馈来源也不一样，一个靠人类标注员，一个是AI自我监督。训练目标更是天差地别：RLHF要符合人类偏好，宪法AI要符合明确原则。可解释性呢？RLHF是黑盒，你不知道为什么它这样回答；宪法AI是白盒，基于明确规则。阿谀奉承的风险，RLHF高，宪法AI低。

实验数据也证明了这一点。Anthropic的论文显示，宪法AI训练的模型在"无害性"上显著优于人类反馈，同时保持了高帮助性。更有意思的是，当让人类对比AI反馈和人类反馈的输出时，人类的偏好几乎是50:50——也就是说，**用AI反馈训练出来的模型，和用人类反馈训练的质量相当，但成本低得多**。

这就是Claude的第一个本质：它不是靠"讨好用户"训练出来的，而是靠"遵守原则"。

所以你会发现，当你让Claude做一些灰色地带的事，它会明确拒绝，并且告诉你为什么。这不是它"不够智能"，是它的设计哲学就这样。

## HHH原则：Claude的三条价值观

Anthropic给Claude定了三个目标，合起来叫HHH：有帮助（Helpful）、诚实（Honest）、无害（Harmless）。

听起来很虚？其实落实到产品上，就是一系列具体的技术决策。

### Helpful：不只是"能干活"

很多人以为"有帮助"就是"功能强大"。其实不是。

Claude 4.5在软件工程基准测试（SWE-bench）上的得分之所以高，不是因为它能写更多代码，而是因为它理解你想做什么，然后给出"生产就绪"的方案——不是原型，是真正能用的。

你看对比。假设让模型"写一个用户登录功能"，GPT可能直接给你一段代码，能跑，但没有错误处理、没有安全考虑、没有数据库优化。Claude会问你："你用的什么数据库？需要支持OAuth吗？要不要加上速率限制？"然后给你一个完整的、考虑了实际部署的方案。

这背后是Claude对"有帮助"的理解：**不是完成任务，而是解决问题**。

### Honest：承认"我不知道"的勇气

这可能是Claude最反直觉的地方。

GPT的训练目标是"生成流畅、听起来合理的文本"，所以当它不确定时，它会编一个听起来像真的的答案——这就是"幻觉"的根源。Claude的训练目标是"遵守原则"，而原则之一就是"信息不确定时，诚实说不知道"。

实测中你会发现，问Claude一个冷门技术问题，它可能说"这个我不确定，建议你查阅XX官方文档"。问GPT同样的问题，它会给你一个看起来很专业、实际上错漏百出的答案。

哪个更"有帮助"？表面上看是GPT，因为它给了答案。但长远来看是Claude，因为它避免了误导。

### Harmless：不是"不敢做"，是"不该做"

Claude的拒绝率比GPT高，这是公认的。但这不是缺陷，这是设计。

Anthropic的数据显示，Claude Sonnet 4.5是"对齐程度最高的前沿AI模型"，在"阿谀奉承（Sycophancy）"和"欺骗"指标上都比之前的模型低。什么意思？它不会因为你是用户就迁就你，也不会为了完成任务而撒谎。

这在某些场景下很烦人，比如你想让它帮你写份"夸张但不算说谎"的简历，它可能拒绝。但在另一些场景下，这是救命的，比如你让它评估一个技术方案，它会直接指出风险，而不是为了讨好你说"没问题"。

## 模型家族：不是"大中小杯"，是场景化设计

很多人以为Claude的模型就是简单的"性能递减"——Opus最强但最贵，Sonnet中等，Haiku最快但最弱。

不对。Claude的模型家族是**按场景设计的**，不是简单的性能分级。

### Sonnet 4.5：复杂智能体+编码

定位是长时间自主工作的智能体（Agent）。可以连续工作30小时，而Opus 4只能撑7小时。支持并行工具调用（Parallel Tool Use），能同时发起多个推测性搜索、并行读取多个文件。在金融、法律、医疗、STEM领域的表现显著优于Opus 4.1。

定价3美元/百万输入tokens，15美元/百万输出tokens。

适用场景是需要长时间、多步骤推理的复杂任务——比如自动化代码审查、复杂的数据分析、多轮需求挖掘。

### Opus 4.1：专业复杂任务

定位是需要极高精确度的专业工作。软件工程基准测试验证集（SWE-bench Verified）达到74.5%，支持多文件代码重构，擅长深度研究和数据分析、细节追踪。

定价15美元/百万输入tokens，75美元/百万输出tokens。

适用场景是大型代码库的精确修复、法律文档的细节分析、需要深度推理的学术研究。

### Haiku 3.5：高速响应

定位是实时交互场景。响应速度最快，成本最低（0.80美元/百万输入tokens）。

适用场景是客服机器人、实时内容审核、需要大量并发请求的应用。

注意到没有？**这不是"大中小杯咖啡"，而是"意式浓缩、美式、拿铁"——不同的产品，服务不同的需求**。

如果你用Haiku做深度代码重构，会失望。如果你用Opus做客服聊天，会破产。选对场景，才能发挥价值。

## 所以，Claude到底是什么？

现在可以回答这个问题了。

Claude不是"另一个ChatGPT"。

它是**一套基于宪法AI（Constitutional AI）训练的、遵循HHH原则的、针对不同场景优化的AI模型家族**。

它的核心差异不在于"更强"或"更弱"，而在于三个点：训练哲学不同——不是讨好用户，而是遵守原则；价值观明确——Helpful、Honest、Harmless；场景化设计——不是一个模型打天下，而是不同模型服务不同场景。

所以，如果你需要长时间、多步骤的自主工作（比如复杂的代码项目），选Sonnet 4.5。如果你需要极高精确度的专业任务（比如大型代码库重构），选Opus 4.1。如果你需要快速响应的实时交互（比如客服聊天），选Haiku 3.5。如果你需要创意头脑风暴、写作辅助、闲聊陪伴……那GPT可能更合适。

理解Claude的本质，不是为了"站队"说它比GPT强，而是为了**在对的场景用对的工具**。

这个系列接下来的文章，会继续深入Claude的核心能力：上下文管理、Extended Thinking、Tool Use、Vision……一步步把这个"不一样的AI"拆解清楚。

---

## 深入交流

想要深入交流AI实践经验？添加微信，一起探讨AI时代的无限可能！

![微信二维码](../assets/二维码.jpg)

> ### 关于本人 ｜ 黄彦湘
> 深耕互联网行业9年，专注前端开发技术方向，现为广州执业律师，同时兼备专利代理师资质。基于丰富的技术背景和法律实践经验，现为深度实践（Deepractice）社区核心贡献者，致力于推动AI深度实践在法律、小说创作等多元领域的创新应用与探索。
>
> **全网同名**：明易AI实践

> ### 关于深度实践
> Deepractice 深度实践 致力于成为AI时代的标准制定者，基于开源生态，为AI应用提供标准化基础设施。
> * 📧 **联系我们**：sean@deepracticex.com
> * 🌐 **官网**：deepractice.ai
> * 💻 **GitHub**：[https://github.com/Deepractice](https://github.com/Deepractice)
