# Pattern | Claude都出4.5了，你还不懂——它能做什么，不能做什么

上篇讲了Claude的本质——基于宪法AI（Constitutional AI）训练，遵循HHH原则，按场景设计模型家族。

但这些都是"设计哲学"层面的东西。真正用Claude的时候，你需要知道：它到底能干什么？哪些事情它干不了？边界在哪？

很多人对Claude的认知停留在"能写代码"、"能分析图片"这种模糊描述上。这导致两个极端：要么高估它，让它做根本做不到的事，然后失望；要么低估它，只用它写写文案，完全浪费了它的能力。

这篇文章就把Claude的能力边界讲清楚。

## 文本生成：不只是"写字"

Claude最基础的能力是文本生成。但"文本生成"这四个字太泛了，具体能做什么？

Anthropic把Claude的文本能力分成8个方向：

**内容创作**：写文章、博客、营销文案、产品描述。这是最常见的用法，但也是最容易浪费的——如果你只用Claude写营销号式的"震惊！"标题，那纯属大材小用。

**对话和聊天**：构建客服机器人、虚拟助手。关键不在于"能聊天"，而在于Claude能理解上下文、记住之前的对话内容（Memory Tool）、保持一致的角色（HHH原则）。

**数据和文本处理**：分类、总结、翻译、格式转换。这里Claude有个很强的能力——Citation（引用）机制，能精确告诉你答案来自哪段文本、哪一页PDF，不是那种"大概在第3页"的模糊回答。

**代码和技术工作**：写代码、调试、解释代码。Sonnet 4.5在软件工程基准测试验证集（SWE-bench Verified）上拿到了业界最高分，不是因为它能写更多代码，而是因为它能连续工作30小时、并行调用多个工具、理解大型代码库的结构。

**视觉处理**：分析图片、处理PDF、提取信息。这个下面单独讲，因为限制很多。

**研究与分析**：研究复杂主题、分析数据、提供洞察。Opus 4.1在这方面特别强，擅长细节追踪，能处理那种"200页法律文件里找出所有潜在风险"的任务。

**个性化和角色扮演**：模拟特定角色、适配语气和风格。这背后是System Prompt + Constitutional AI的组合，能让Claude保持稳定的角色定位，不像某些模型聊着聊着就"人设崩了"。

**教育和学习**：解释概念、回答问题、生成练习题。Claude的"诚实（Honest）"原则在这里很关键——当它不确定时会明确说"我不知道"，而不是编一个听起来像真的的答案。

但你注意到没有？这8个方向都有个共同特点：**不是单纯生成文本，而是理解、推理、然后生成**。

这就是Claude和传统"生成式AI"的区别。GPT的核心目标是"生成流畅、听起来合理的文本"，Claude的核心目标是"遵守原则、解决问题"。

## Vision：能看，但看不清所有东西

Claude 3开始支持Vision，能分析图片、处理PDF。但这个能力有很多边界，Anthropic说得很明确。

### 能做什么

分析图表、提取图片中的文字、理解图片内容、回答关于图片的问题。处理PDF时可以做页码级别的引用（Citations），告诉你答案来自第几页。

对于图片，Claude能给出"大概的物体数量"，能描述"相对位置"（杯子在桌子上），能识别图表类型和数据趋势。

### 不能做什么

**不能识别特定人物**。这是政策限制，不是技术限制。你给Claude一张照片问"这是谁"，它会拒绝回答。Anthropic的立场很坚定——涉及隐私和合规的事，不做。

**空间推理能力有限**。虽然能描述相对位置，但精确的空间关系、距离测量、复杂的几何推理，都不行。比如给它一张模拟时钟的图，它可能读不准时间；给它一张国际象棋棋盘，它可能说不清楚各个棋子的精确坐标。

**精确计数困难**。能给"大概数量"，但如果图片里有几十上百个小物体，它的计数可能不准。这不是粗心，是Vision模型的通病——图像编码过程中会损失细节。

**不能检测AI生成图片**。你问Claude"这张图是AI画的吗"，它可能猜错。Anthropic的文档明确说了：别指望用它来鉴别真假图片。

为什么有这些限制？因为Vision能力的本质是"图像编码→文本理解"，中间有信息损耗。Claude把图片编码成Tokens（1张图最多1600个tokens），然后基于这些Tokens理解图片。编码过程中，精确的空间信息、小物体的细节、人脸的特征，都会被压缩或丢失。

这不是Claude特有的问题，是所有Vision模型的共同挑战。

## Extended Thinking：推理的革命

Sonnet 4.5有个很强的能力叫Extended Thinking（扩展思考），能让Claude在回答前"思考"更久。

听起来很玄，但背后的原理很硬核：**串行测试时计算（Serial Test-time Compute）**。

什么意思？传统的LLM是"一次性生成"——你问一个问题，模型直接给答案。Extended Thinking是"逐步推理"——模型先生成一段"思考块（Thinking Block）"，在这个块里逐步分析问题、列举可能性、排除错误答案，然后再给出最终答案。

你会问：这有什么用？

Anthropic的数据显示，**数学问题的准确率随着思考Tokens的对数增长**。也就是说，让Claude"多想想"，准确率会明显提升。特别是那种需要多步推理的复杂问题——比如数学证明、逻辑推理、代码调试——Extended Thinking能把准确率提升一大截。

Extended Thinking在Claude 3.7首次引入，Claude 4做了优化——思考块会自动生成总结，避免占用太多上下文。

但有个限制：**Extended Thinking不兼容temperature修改**。Temperature是控制"随机性"的参数，Extended Thinking需要"确定性"的推理过程，所以不能调。

还有个经济学问题：思考Tokens是要收费的。Sonnet 4.5的思考Token定价是10美元/百万Tokens，Opus 4是40美元/百万Tokens。如果你让Claude"深度思考"一个复杂问题，可能会消耗几千个思考Tokens，成本不低。

## Tool Use：能力的边界在于工具

Claude本身是个语言模型，但Anthropic给它加了9个Server Tools，让它能"调用外部能力"。

这9个工具是：

**Computer Use**：控制桌面，操作鼠标键盘，截屏，跨应用自动化。这是最夸张的能力——Claude可以像人一样操作电脑，打开浏览器、填表单、点按钮。但限制也很明确：需要沙盒环境、限制互联网访问、需要人工确认关键操作。

**MCP Connector**：通过模型上下文协议（MCP）连接远程服务器，扩展Claude的能力。这不是"调用API"，是"协议化地提供上下文"。

**Memory Tool**：跨对话记忆，把重要信息保存到`/memories`目录，下次对话能直接调取。这解决了"上下文窗口有限"的问题——不是把所有东西都塞进200K的上下文，而是把关键信息持久化保存。

**Web Search & Fetch**：搜索网页、抓取内容。这是Claude官方的RAG（检索增强生成）实现，10美元/1000次搜索。

**Code Execution & Bash**：执行Python代码、运行Bash命令。沙盒环境，安全隔离。

**Text Editor**：编辑文本文件。

**PDF Support**：处理PDF，支持页码级别的引用。

这些工具的本质不是"让Claude变万能"，而是"让Claude能调用专业能力"。Claude本身不会"上网"，但可以调用Web Search工具。Claude本身不会"操作电脑"，但可以调用Computer Use工具。

关键在于：**工具定义了能力边界**。如果没有对应的工具，Claude做不了。比如你想让Claude"发微信"，但Anthropic没提供"微信工具"，那就做不了（除非你自己开发MCP服务器）。

## SWE-bench Verified：为什么这个测试很重要

Sonnet 4.5在软件工程基准测试验证集（SWE-bench Verified）上拿了业界最高分，很多报道都在吹这个数据。但SWE-bench到底是什么？为什么重要？

SWE-bench是个"真实世界软件工程任务"的测试集。不是那种"写个冒泡排序"的简单题，而是"修复GitHub上真实项目的真实Bug"。

SWE-bench Verified是SWE-bench的"验证子集"——筛选掉了那些有歧义、有争议、难以自动评估的任务，只保留了"明确可验证"的高质量任务。

为什么重要？因为这是目前最接近"真实开发工作"的AI评估标准。它测的不是"能不能写代码"，而是"能不能理解大型代码库、定位问题、写出生产级别的修复"。

Sonnet 4.5的高分说明了什么？说明它能：
- 理解复杂的代码库结构
- 定位Bug的根源
- 写出符合项目规范的代码
- 处理多文件的代码重构

但SWE-bench也有局限性。它测的是"修Bug"，不测"设计架构"、"写文档"、"code review"。所以高分不代表Claude能完全替代程序员，只代表它在"Bug修复"这个特定任务上很强。

## 限制与边界：Claude不是万能的

讲完能做的，现在讲不能做的。

### 幻觉问题

所有LLM都有幻觉问题——模型会"编造"听起来合理但实际错误的信息。Claude的宪法AI训练方式能减少幻觉（因为有"诚实"原则），但不能消除。

Anthropic的建议是：**不要在需要100%准确的场景用Claude**。比如医疗诊断、法律意见、金融决策——这些场景的错误成本太高，AI的"大概对"不够用。

### 上下文窗口的真实含义

Claude支持200K上下文，1M上下文在Beta测试中。听起来很大，但**200K上下文≠能理解200K内容**。

为什么？因为"上下文窗口"只是"工作记忆"，不是"理解能力"。你给Claude一个200K的文档，它能把这个文档"装进记忆"，但不代表它能完美理解每一段、记住每个细节。

Anthropic的文档里提到，即使有200K上下文，也建议用Context Editing（上下文编辑）机制自动清理不重要的内容，保留关键信息。为什么？因为上下文越长，模型的"注意力"越分散，理解质量会下降。

### 不能做医疗诊断

Anthropic明确说了：Claude不能用于医疗诊断。这不只是法律免责声明，是技术现实。

医疗诊断需要：100%的准确性（幻觉不能接受）、对罕见病的识别（训练数据可能不够）、对患者个体差异的判断（LLM不擅长）、对责任的承担（AI没法负责）。

Claude可以做什么？辅助研究、总结文献、解释医学术语、生成患者教育材料。但诊断？不行。

### 不能完全自主

Sonnet 4.5能"连续工作30小时"，听起来像是"完全自主的AI Agent"。其实不是。

它能在给定的任务范围内自主工作——比如"修复这个代码库的所有类型错误"——但不能"自己决定要做什么"。你需要给明确的任务、明确的边界、明确的工具权限。

Computer Use工具最明显：虽然Claude能操作电脑，但Anthropic强烈建议"在沙盒环境运行"、"限制互联网访问"、"关键操作需要人工确认"。为什么？因为AI可能犯错，自主操作的风险太高。

## 所以，Claude的能力边界在哪？

现在可以总结了。

Claude的能力边界不是"技术上能不能做"，而是"在什么场景下、用什么方式、能做到什么程度"。

**文本生成**：能做高质量内容创作、代码编写、数据分析，但不能保证100%准确，不能完全替代人类判断。

**Vision**：能分析图片、处理PDF，但不能识别人脸、不能精确空间推理、不能完美计数小物体。

**Extended Thinking**：能提升复杂推理的准确率，但需要额外的思考Tokens成本，不适合简单任务。

**Tool Use**：能调用9种官方工具扩展能力，但边界由工具定义——没有对应工具，就做不了。

**自主工作**：能在明确任务范围内连续工作，但不能完全自主决策，需要人类监督。

理解这些边界，你才能真正用好Claude。不是问"Claude能不能做XX"，而是问"在什么场景下、用什么工具、给什么限制，Claude能做到什么程度"。

下一篇会讲Claude的产品矩阵——Web、API、Projects、Claude Code——为什么Anthropic要做这么多产品，它们的架构设计有什么差异。

---

## 深入交流

想要深入交流AI实践经验？添加微信，一起探讨AI时代的无限可能！

![微信二维码](../assets/二维码.jpg)

> ### 关于本人 ｜ 黄彦湘
> 深耕互联网行业9年，专注前端开发技术方向，现为广州执业律师，同时兼备专利代理师资质。基于丰富的技术背景和法律实践经验，现为深度实践（Deepractice）社区核心贡献者，致力于推动AI深度实践在法律、小说创作等多元领域的创新应用与探索。
>
> **全网同名**：明易AI实践

> ### 关于深度实践
> Deepractice 深度实践 致力于成为AI时代的标准制定者，基于开源生态，为AI应用提供标准化基础设施。
> * 📧 **联系我们**：sean@deepracticex.com
> * 🌐 **官网**：deepractice.ai
> * 💻 **GitHub**：[https://github.com/Deepractice](https://github.com/Deepractice)
